
This paper presents our efforts to reproduce the results achieved by the authors of the original article \cite{WenZha:2018}. We follow the steps and models described in their article and the same public data sets of EEG Signals. Epilepsy affects more than 65 million people globally, and EEG Signals are critical to analyze and recognize epilepsy. Despite recent efforts/advances, it is still challenging to extract useful information from these signals and select useful features in a diagnostic application. We construct a deep convolution network and autoencoders-based model (AE-CDNN) in order to perform unsupervised feature learning. We use the AE-CDNN to extract the features of the available data sets, and then we use some common classifiers to classify the features. The results obtained demonstrate that the proposed AE-CDNN outperforms the traditional feature extraction based classification techniques by achieving better accuracy of classification.

\section{Introduction}

The paper {\it Deep Convolution Neural Network and Autoencoders-Based Unsupervised Feature Learning of EEG Signals} \cite{WenZha:2018} presents an unsupervised approach to learn from EEG signals of epilepsy patients. It uses dimension reduction algorithms to extract features since the original data is high-dimensional. Then, it uses different common classifiers to classify the features obtained. 	

We re-implement the author's method from scratch as best as we could by using only the paper as instruction. In addition, we get new results by combining the proposal classifiers into a classifier by set voting.

This paper is organized as follows: Section \ref{sec:propose} introduces the methodological proposal employed, and their differences from \cite{WenZha:2018}. Section \ref{sec:metho} lists the experimental validation process using epilepsy datasets. Section \ref{sec:resu} presents the corresponding results and analyzes our approach. Finally, conclusions were summarized in Section \ref{sec:concl}.



\section{Methodology Proposal}
\label{sec:propose}

In this section, we describe implementation details, as the core is the reproducible aspect of our reference article. We introduce the idea and implementation of autoencoder/feature learning and our version of the model in \cite{WenZha:2018}, explaining the differences we have made in the original model.

\subsection{Implementation Details}\label{subsec:implementation}

We decided to reproduce the implementation described in the article using Keras \cite{chollet2018keras} with TensorFlow  \cite{tensorflow} as backend. Our repository includes a list of all the required libraries employed in acquiring the datasets and running the model (the original and the proposed one). Using to the methodology proposed in \cite{Fuente:2019}, we store all the checkpoints for the trained models for reproduction purposes. Besides that, the training logs can be visualized using the TensorBoard tool.

Given the lack of information about some implementation details in the original paper, some assumptions or cuts are made: 

\begin{itemize}
\item The number of epoch in the AutoEnconder is assumed to be $5000$; 
\item The number of samples per batch size is assumed to be $256$; 
\item The last column of the Bonn University EEG dataset was removed since the authors of \cite{WenZha:2018} used $4096$ features and in this database there are $4097$ column. 
\item In the Children's Hospital of Boston EEG database we use the channel reported by the author to train the AutoEncoder;
\item The loss function presented in equation $12$ in \cite{WenZha:2018} was implemented; 
\item The value of the seeds used in all classifiers, data splitting and elsewhere was $42$; 
\item The train-validation ratio was $80\%-20\%$ for the AutoEnconder. For the classifiers we used 5 or 10-fold cross-validation 
\item We resized the values using the MinMax method, before the classification process.
\item The classifier presented in the final subsection (NN2) was not reproduced for lack of information; 
\end{itemize}

We performed the experiments using a computer with an Intel Core i7-5930K at 3.50 GHz CPU and two GPUs: Nvidia Quadro K5200 and GeForce GTX 970. We also executed some experiments using a Nvidia Titan X GPU.


\subsection{AutoEncoders}

The autoencoder implemented is a specific case of neural network structure. It is formed by set three layers, an encoder layer, an decoder layer and a hidden layer. The training is done to set the weights of the hidden layer to force the input layer and output layer to be as close to each other as possible. Our features are extracted from the hidden layer, which reduces the dimension of data. For more details about the encoding and decoding functions see the Section 2 in the original article \cite{WenZha:2018}.

\subsection{Feature Learning Model}

In this subsection, we will omit equations and minor details (for complete information, see \cite{Shoeb,emami2019autoencoding}). Since we have the dimension reduced by autoencoder we focus on the next challenge: how to obtain effective features from the EEG signals. The AE-CDNN implemented follows the steps:

\begin{itemize}
\item Encoder: sample input, convolution layer, down-sampling layer, reshape operation, full connection layer, and feature coding.
\item Decoder: feature coding as input, full 
connection layer, reshape operation, deconvolution layer, up-sampling layer and sample reconstruction
\end{itemize}

Basically, the convolution layer acts as our feature extractor. It performs many successive convolution calculations over the input data allowing the extraction of useful features from the data. The pooling layer is a down-sampling method which reduces data dimension. In the convolution and pooling layer windows are used to slide and extract the feature maps. These intervals do not overlap each other, and with them we obtain the pooled feature maps. The feature sizes tested were $m \in \{2, 4, 8, 16, 32, 64, 128, \}$.


The convolution and pooling operations can be iterated multiple times. Reshape operation uses the pooled feature maps to construct an one-dimension vector and a full-connection layer to transform this one-dimension vector. 

Considering $x$ as the input and $y$ as the output, now we need to re-transform the one-dimension vector which will generate the $y$ output, recall we want to minimize the difference between $x$ and $y$ and we have the following equation to calculate loss Mean Absolute Error:

$$\text{Loss MAE}= \frac{1}{N} \sum_{i=1}^N |x^{(i)} - y^{(i)}| .$$


In addition, we have also used/implemented one more loss function, Mean Absolute Average Error - MAAE. The formula presented in the original article by \cite{WenZha:2018} differs from the Mean Absolute Percentage Error - MAPE formula, despite having similar intuitions. Thus, we chose to implement this loss function, and we have not found its use elsewhere, the difference between the loss functions is only in the fact that one takes in the denominator the value per $x^{(i)}$ and the other takes the average ${\bar x^{(i)}}$, that are contained below:

$$\text{Loss MAAE}= \frac{1}{N} \sum_{i=1}^N \frac{|x^{(i)} - y^{(i)}|}{{\bar x^{(i)}}} .$$

Note that we refer to \cite{WenZha:2018}'s AE-CDNN-L1 as {\it Loss-MAE} and to \cite{WenZha:2018}'s AE-CDNN-L2 as {\it Loss-MAAE}

\subsection{Classification}

Since we have extracted the features with reduced dimension, we use supervised learning models on these features in order to classify the EEG signals. We evaluate each classifier and then we compare the results obtained with each one. The classical classifiers used are: K-Nearest Neighbors (K-NN), Support-Vector Machine - Linear Kernel and Radial Basis Kernel (SVM1, SVM2), Decision Tree (DT), Random Forest (RT), Multilayer Neural Network (MLP), Adaptive Boosting Algorithm (ADB) and Gaussian Naive Bayesian (GNB).
 
\section{Experimental Methodology}
\label{sec:metho}
In this paper, as in our reference paper \cite{WenZha:2018}, we use unsupervised learning method in EEG signals in order to obtain useful features. This process is needed because the original data is high-dimensional. By using the auto-encoder, we can extract features with reduced dimension. As the original authors we may refer to Bonn University EEG database simply as dataset 1 and to Children's Hospital of Boston EEG database simply as dataset 2.

\subsection{Bonn University EEG database}

We can use different approaches to detect epileptic crisis. Then, to obtain a comparative measure, we verify our outputs using the method described in \ref{sec:propose} and the original one showed in Section \cite{WenZha:2018}. This database is public and was published by \cite{andrzejak}. The study groups were the control, inter-ictal and ictal distributed into five sets (denotated A-E). Each containing $100$ records of $23.6$ seconds duration and frequency of $173.6$ Hz on a single channel, with $12$-bit resolution. Each data segment has 4097 samples. These recordings underwent a pre-processing in which the signals had a band filter between $0.53$ to $40$ Hz. There was also the removal of artifacts such as muscle movements or flicker movements.

We used labels A, B, C, D and E for the subsets. Set A corresponds to open-eye activity and subset B to closed-eye activity of 5 healthy volunteers. The subsets C and D have interictal epileptiform activity from 5 epileptic patients. And E contains signals during epileptic patients' seizure (ictal intervals). According to \cite{kamath2015analysis}, this dataset is a compilation of recordings under different conditions.

\subsection{Children's Hospital of Boston EEG database}

The second database, also public, contains the EEG signals from a Children's Hospital of Boston
\cite{Shoeb}. It was recorded by measuring the brain's electrical activity to obtain EEG signals by connecting multiple electrodes to the patientsâ€™ scalp. The data incorporates the EEG signals of 23 children with refractory epilepsy.

This database, built in partnership with the Massachusetts Institute of Technology (MIT), has $5$ men and $18$ women between $3$ and $22$ years. The frequency range is $256$ Hz with $16$ resolution bits. Most patients contain $ 23 $ channels and some with $24$ channels. In contrast to the first set of data, we have multiple channels here, then we need to select channels. The selection followed the methodology used in \cite{shoeb2009application}, which analyzes the variance of each patient, and after that, chooses the channel of greater variance to represent that individual. The channel reported by the authors was $\text{FT9-FT10}$.

In the data of the first ten patients, we chose two hundred $200$ windows with a size of $4096$ from the epileptic seizures and another two hundred $200$ when there are no epileptic seizures.

\begin{comment}
\subsection{Performance Measures}

According to \cite{roy2019deep}, most of the state-of-the-art systems for epilepsy use the metrics defined \ref{table:metrics}. The adaptation of these metrics for evaluating our system contributes to fair comparison with state-of-the-art systems. The definitions of these metrics are given in Table \ref{table:metrics}.

\begin{table}[!ht]
\centering
\begin{tabular}{ccccc}
\hline
 \textbf{Acurracy} & \textbf{Precision} & \textbf{Specificity} & \textbf{Sensitivity} & \textbf{F-Measure} \\ \hline
 $\frac{TP+TN}{TP+TN+FP+FN}$ & $\frac{TP}{TP+FP}$ & $\frac{TN}{TN+FP}$ & $\frac{TP}{FN+TP}$ & $\frac{2\cdot Precision \cdot Sensitivity}{Sensitivity+Precision}$\\ \hline
\end{tabular}
\caption{Use of Metrics and Definition in our paper. Only the Acurracy was considered in \cite{WenZha:2018}.}
\label{table:metrics}
\end{table}

\noindent where False Negatives - FN is the number of epileptic cases, which are predicted as control, True Positives - TP is the number of epileptic cases, which are predicted as epileptic, True Negative - TN is the number of control case that is predicted as control and False Positives - FP is the number of control cases that are identified as epileptic by the system. 

In addition, there was also the AUC-ROC (Area Under The Curve - Receiver Operating Characteristic) defined as the cumulative distribution function of the true positive rate vs the false-negative rate denoted by a threshold.
\end{comment}



\section{Results and Discussion}
\label{sec:resu}

In this section, we present our reproduction results. In the first subsection we analyze the variance present in the channels. The second contains the reproduction of all possible tables and figures, with a discussion of the reasons for the differences. 

\subsection{Checking the Variance} 

According to the original authors, the choice of the channel in the Children's Hospital of Boston EEG database observed the variance present in the channels. For that, they followed the methodology: 1) calculate the variance of each channel in each sample, and select the channel with the maximum variance for each sample; 2) count the number of times each channel was selected; 3) select the channel the highest count among the first $10$ patients.

Following the methodology presented above we found a different channel from the one found in \cite{WenZha:2018}. So we decided to explore other scenarios. Note that the variance can be calculated based on each sample, each person or the full database. Despite the fact the authors in \cite{WenZha:2018} mentioned explicitly they used the variance of each sample, since we found a different result we explored the other possibilities (per person and the full database).

Thereby, we modeled the three scenarios. In the first, we analyzed each recording file of the dataset as a sample, having an average length of $921600$ points referring to the recorded $3600s$. For each file we computed and selected the electrode with most variance. We counted the number of times each electrode was selected. The results obtained can be seen in the Figure \ref{}:

In the second scenario, we understand that each sample is accumulated per person with all his recordings. So the variance was calculated in parallel in the files and combined for each person. For each person, we count the occurrence of the channel with more variance. As shown in Figure \ref{fig:variance_per_person}.

Finally, as a final scenario, we calculate the cumulative variance across all people and all records, thus, we did not perform a sampling process. In other words, we put all the files together and calculate the variance as if it were a single record. For this, we compute the variance, number of points and average per channel in each file and accumulate through the cumulative variance calculation algorithm. The result of this analysis approach can be seen in Figure \ref{fig:variance_all_files}.



\begin{figure}[!ht]
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth]{figure/variance_per_file.pdf}
\caption{Accumulated variance per sample, considering a sample as each recording file.}
\label{fig:variance_per_file}
\end{subfigure}\\
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth]{figure/variance_per_person.pdf}
\caption{Accumulated variance per sample, considering a sample as being all the recordings of each person.}
\label{fig:variance_per_person}
\end{subfigure}\\
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth]{figure/variance_all.pdf}
\caption{Total accumulated variance in the first ten people of the dataset, as granular as possible.}
\label{fig:variance_all_files}
\end{subfigure}\\
\caption{Three scenarios modeled to calculate the variance.}
\end{figure}


The results obtained in the first, second and third scenarios were not consistent with those reported by the author, in any of them we found the channel FT9-FT10 as the one appearing most for the first $10$ patients. There is also another possible scenario, however, it is not reproducible, where the authors randomly sampled the dataset and evaluated the variance.

Given the associated uncertainty, we decided to repeat the choice of the channel $FT9-FT10$, although this is not the one with the most variance in the modeled scenarios.


\subsection{Reproduction of the values reported by the original author}

The results obtained in our reproduction experiment, for the first dataset, are presented in Accuracy Tables \ref{table:accuracy_boon_mae-reproduction} and \ref{table:accuracy_boon_maae-reproduction} and the differences between results can be seen in Figures \ref{fig:acc-AE-CDNN-MAE-d1} and \ref{fig:acc-AE-CDNN-MAAE-d1}. We employed the same methodology as the one used in the original paper, performing a $5$-fold cross-validation for each classifier, and we show the mean values.
In the first column, we have the encoded hidden layer size\footnote{Denoted here as Dimension and is equivalent to the variable $m$ in the original paper.} after the feature learning process. The remaining columns are the average values obtained in the classification using the latent space as input. For each table reproduced, we also present the original result and the difference between them.


\input{table/accuracy_boon_mae-reproduction.tex}

\input{table/accuracy_boon_maae-reproduction.tex}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/table_2.pdf}
  \caption{Classification Accuracy Results, for AE-CDNN-MAE as feature learning, in Dataset 1. Reproduced and Difference between the values contains in Table 2 in \cite{WenZha:2018}. We use the same hyperparameters in the classifiers.}
\label{fig:acc-AE-CDNN-MAE-d1}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/table_3.pdf}
  \caption{Classification Accuracy Results, for AE-CDNN-MAAE as feature learning, in Dataset 1. Reproduced and Difference between the values contains in Table 3 in \cite{WenZha:2018}. We use the same hyperparameters in the classifiers.}
\label{fig:acc-AE-CDNN-MAAE-d1}
\end{figure}

We can perceive some differences when compared to the original results. In Table \ref{table:accuracy_boon_mae-reproduction}, we obtained the best average with a dimension equal to 64, while the original document obtained the best average when the dimension is equal to 128. The original document obtained higher accuracy values in most cases, even though when the dimension is equal to 2 or 4, our accuracy values are higher. The best precision in our article and in the original article was obtained by the random forest algorithm.


\input{table/accuracy_chbmit_mae-reproduction.tex}

\input{table/accuracy_chbmit_maae-reproduction.tex}



\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/table_4.pdf}
  \caption{Classification Accuracy Results, for AE-CDNN-MAE as feature learning, in Dataset 2. Reproduced and Difference between the values contains in Table 4 in \cite{WenZha:2018}. We use the same hyperparameters in the classifiers}
\label{fig:acc-AE-CDNN-MAE-d2}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/table_5.pdf}
  \caption{Classification Accuracy Results, for AE-CDNN-MAAE as feature learning, in Dataset 2. Reproduced and Difference between the values contains in Table 5 in \cite{WenZha:2018}. We use the same hyperparameters in the classifiers}
\label{fig:acc-AE-CDNN-MAAE-d2}
\end{figure}

Note that Tables $3$ and $4$ in \cite{WenZha:2018} have the exact same results, considering they contain results for different datasets and loss functions it seems to be an error in which the original authors used the same image in both cases by mistake. As our attempts to contact the original authors failed, we present our own results and comparisons. Therefore, the comparison of these two tables may be affected by this issue.

Considering Dataset 2 and Tables \ref{table:accuracy_chbmit_mae-reproduction}, \ref{table:accuracy_chbmit_maae-reproduction} we obtained similar results when compared with the results obtained by the original authors \cite{WenZha:2018}. In general, the original paper obtained a maximum accuracy greater than those obtained by our reproduction implementation, but the average and unique values per dimension are close in most cases considering both functions AE-CDNN-MAE and AE-CDNN-MAAE. However, for Dataset 1 the accuracy values obtained in this paper are significantly lower than the ones obtained by the original paper considering both AE-CDNN-MAE and AE-CDNN-MAAE, as shown in Figure \ref{fig:average}. 


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/average-MAE-MAAE.pdf}
  \caption{Average Accuracy Results of AE-CDNN-MAE and AE-CDNN-MAAE, with different dimension values in the two dataset. Reproduction of Figure 7 in \cite{WenZha:2018}. }
\label{fig:average}
\end{figure}

As already shown in the Tables, the results obtained in the reproduction were similar. Analyzing the behavior of the different loss functions, we see that the MAAE function does not always obtain superior results than the MAE function. The same is observed in the original article, but their average accuracy obtained are higher than those obtained by our reproduction. 


\begin{comment}

Similarly, when we analyze the loss function MAAE and MAPE, in Figure \ref{fig:average-maae-mape}, we have that the behavior of both is not very divergent, being MAAE generating a higher accuracy in the first dataset. In the second dataset, MAPE has a more stable behavior and generates greater accuracy. 


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/average-MAAE-MAPE.pdf}
  \caption{Average Accuracy Results of AE-CDNN-MAAE and AE-CDNN-MAPE in the two dataset.}
\label{fig:average-maae-mape}
\end{figure}

\end{comment}

For each $k$-fold, we observed the accuracy values are following the class balance in the test. It's indicating the classification methods there are not better than random classifications that follow the classes' percentage. We observed in Tables \ref{table:acc-mae-boon-fold-2, table:acc-maae-boon-fold-2.tex} the result for the accuracy inspection, for the cross validation, for $m = 2$. Analyzing the accuracy obtained by classifiers in Tables \ref{table:acc-mae-boon-fold-2} and \ref{table:acc-maae-boon-fold-2} we observe the values obtained by AE-CDNN-MAAE and AE-CDNN-MAE are close, however the function AE-CDNN-MAAE obtained smoothly better results and with less variation, in general.

\input{table/acc-mae-boon-fold-2.tex}

\input{table/acc-maae-boon-fold-2.tex}

In the original paper we observe similar differences between the two functions, the results for AE-CDNN-MAAE are slightly better for most classifiers. But considering \textbf{gaussian\_nb}, for example, the function AE-CDNN-MAAE obtained much better results compared with AE-CDNN-MAE. Although the results in original paper also have few variations for the classifiers \textbf{svm\_linear}, \textbf{svm\_radial} and \textbf{multi\_layer} we had no variations in these classifers for the autoencoder AE-CDNN-MAAE.

In the second dataset, in Tables \ref{table:acc-mae-chbmit-fold-2, table:acc-maae-chbmit-fold-2.tex}, when analyzing by fold we had worse results than those reported by the authors. However, the results are consistent with the hypothesis that during the process there was no feature learning. Also given the balance of this second dataset, we have that all methods do not present a better result than random chance.

\input{table/acc-mae-chbmit-fold-2.tex}

\input{table/acc-maae-chbmit-fold-2.tex}

When analyzing the reduced values by class, specifically with $m = 4$, we note that $3$ of the $4$ attributes are $0$, in the best scenario, indicating that there was no learning in Auto Encoder to distinguish the behavior by class. This bad representation of latent space occurs regardless of the loss function.


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/feature_distribution_4.pdf}
  \caption{Feature Distribution of AE-CDNN-MAE and AE-CDNN-MAAE, with $m=4$, in the first dataset. Reproduction of Figure 8 in \cite{WenZha:2018}.  }
\label{fig:feature_distribution_4}
\end{figure}


When we analyze the behavior of the loss functions at the epoch, in Figure \ref{fig:change_loss_mae_maae}, in the first dataset, we note that our loss values are much higher than those obtained by the authors. 


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/change_loss_mae_maae.pdf}
  \caption{Change of loss function of AE-CDNN-MAE and AE-CDNN-MAAE, in the first dataset, with $m=4$. Reproduction of Figure 9 in \cite{WenZha:2018}. }
\label{fig:change_loss_mae_maae}
\end{figure}


\begin{comment}

Even assuming that the author used the MAPE loss function, we still do not obtain an adequate result in loss at the epoch, as show the Figure \ref{fig:change_loss_mae_mape}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/change_loss_mae_mape.pdf}
  \caption{Change of loss function of AE-CDNN-MAE and AE-CDNN-MAPE, in the first dataset, with $m=4$. }
\label{fig:change_loss_mae_mape}
\end{figure}

\end{comment}

These differences also occur in the establishment in the baseline methods, indicating that there is some cut in the training set that was not included in this modeling, given the lack of information in the article. In Figure \ref{fig:baseline_methods} we observe similar average accuracy between AE-CDNN-MAE, AE-CDNN-MAAE, PCA and SRP for both datasets. 


\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figure/baseline_methods.pdf}
  \caption{Comparison of accuracy for different loss functions (AE-CDNN-MAE, AE-CDNN-MAAE), and also with baseline (PCA, SRP). Reproduction of Figure 10 in \cite{WenZha:2018}.}
\label{fig:baseline_methods}
\end{figure}

The same is observed in the original article, as well as a similar behavior, but the average accuracy obtained for Dataset 1 are significantly higher than those obtained by our reproduction, as shown in the Table \ref{table:metrics_boon_cv_10}:

\input{table/metrics_boon_cv_10.tex}

When we analyze the result assuming a $10$-fold, we have an increase in the accuracy values for the first dataset, however, still below that reported by the original authors.

\begin{comment}

\subsection{Extension of the values reported by the original author}
 
In Precision Tables \ref{table:precision_boon_mae-reproduction}, \ref{table:precision_boon_maae-reproduction}, \ref{table:precision_chbmit_mae-reproduction} and \ref{table:precision_chbmit_maae-reproduction}, we realize that one of the most precise and specific method was the Gaussian Naive Bayesian; however, when analyzing the behavior in the Sensitivity metric, we do not have a satisfactory result. This indicates that the method pinpoints true negatives rather than true positives. If treated from a medical field, this result is worrying. The cases that the method indicates are true positives; however, this method misses many cases.

We also analyze that we cannot consider Support Vector Machine (Linear and Radial) or Multi-Layer results with the lowest $m$. The result in specificity indicates that the method behaves unwanted, possibly indicating all values as true positives. This rule burdens the medical system because further detection of the seizure requires further investigation for a complete diagnosis of the disease.

By our method, we note that the three metrics indicate that a progression in the number of features generates an improvement in seizure detection. Similar behavior is observed in Multi-Layer, only for Accuracy and Sensitivity, in this case, a beneficial behavior for the application. No trend was observed in the other methods. The average of the methods does not exceed our Ensemble method in almost any scenario. 


\input{table/precision_boon_mae-reproduction.tex}

\input{table/precision_boon_maae-reproduction.tex}


In the first dataset, when we analyze the accuracy we have that the naive bayes Gaussian classifier presents a drop of ($ 40 \%, 23 \% $, for first and second loss respectively) if compared to the accuracy. The average difference, in precision minus accuracy, is $6\%$, indicating that the precision metric achieves slightly higher results on average in the samples. The k\_neighbors classifier is the classifier, in the first loss function, that has the least average difference in results, while the svm\_linear method shows the same result for the second loss set. At the other end, we have the largest variation in both gaussian\_nb data sets.


\input{table/precision_chbmit_mae-reproduction.tex}

\input{table/precision_chbmit_maae-reproduction.tex}

Meanwhile, in the second set of data, generated by the two loss functions, the difference between precision and accuracy is greater in the smallest dimensions, while the values are more stable, and close to the accuracy values in the largest dimensions. Such stability behavior is also observed in the absolute values.


\subsubsection{Specificity and Sensitivity}

When we analyze the specificity, we have that the SVM method, with different kernels, cannot obtain a separation of the hyperspaces of the attributes to distinguish the non-schizoid events. In this way, we have that the classifier cannot distinguish when the person is without epileptic attack. From a medical point of view, there are not so many implications for this, since the weighting of importance is inclined to detect true positives. The SVM sensitivity for these cases, in high dimensions (above $ 32 $) presents reasonable values, approximately $70\%$ in the worst scenarios. In general, the panorama of the accumulated sensitivity indicates that the worst classifiers, regardless of the number of dimensions, are the Gaussian naive bayes, and the \textbf{K}-neighbors for high dimensions. The ensemble classifier has average cumulative sensitivity ($ 71 \% $ in the worst case scenario), with the exception of lower case scenarios $ 2 $.

The performance of the Gaussian classifier may be related to the fact that the inputs are highly dependent on each other, thus violating the method's premise of independence. In the case of the k-neighbors classifier, given the presence of the values $ 0 $ in various dimensions, as shown previously, which can affect the distance assumptions necessary for the method.



\input{table/specificity_boon_mae-reproduction.tex}

\input{table/specificity_boon_maae-reproduction.tex}


\input{table/specificity_chbmit_mae-reproduction.tex}

\input{table/specificity_chbmit_maae-reproduction.tex}



\input{table/sensitivity_boon_mae-reproduction.tex}

\input{table/sensitivity_boon_maae-reproduction.tex}

\input{table/sensitivity_chbmit_mae-reproduction.tex}

\input{table/sensitivity_chbmit_maae-reproduction.tex}

\newpage

\subsubsection{F-measure and ROC-AUC}\label{subsec:fmeasure}

When analyzing the behavior of the methods for the reason of the metrics of $ F-measure $ and $ ROC-AUC $ we have that in the first data set, the SVM and multi\_layer methods present the best results. At the other end, we have the appearance with the naive bayes and $k$ neighbor methods.

We analyze the behavior of the F-measure. The relationship between sensitivity and precision is captured by this measure, as the gaussian\_nb, svm\_linear, k\_neighbors methods did not obtain good results, which is coherent with the accuracy result. The measures generally show close results with each other. 



\input{table/f-measure_boon_mae-reproduction.tex}

\input{table/f-measure_boon_maae-reproduction.tex}

\input{table/f-measure_chbmit_mae-reproduction.tex}

\input{table/f-measure_chbmit_maae-reproduction.tex}


\input{table/roc-auc_boon_mae-reproduction.tex}

\input{table/roc-auc_boon_maae-reproduction.tex}

\input{table/roc-auc_chbmit_mae-reproduction.tex}

\input{table/roc-auc_chbmit_maae-reproduction.tex}

\end{comment}


\newpage

\section{Conclusion}\label{sec:concl}
In this article, we re-implemented the approach proposed in \cite{WenZha:2018} and propose the use of a different classifier. This classification approach, based on deep learning for detecting epileptic seizures using EEG had not been explored previously. We used a Auto-Encoder that allowed us to construct a smaller representation space.

The original authors left some gaps that made it impossible to fully reproduce their experiment. For example, the lack of information about the Section IV (C. Comparison of Classification Application) in the original paper, the sampling process of the first and second data sets and the number of times or batch size (see \ref{subsec:implementation} to check the assumptions we made).

As contribution, the developed code can be easily ported to other tasks. Moreover, it could be used to evaluate other variants of the neural network architecture, techniques for classifying the signals, data augmentation, among other possibilities.  



