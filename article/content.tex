
This paper presents our efforts to reproduce the results achieved by the authors of the original article. We follow the steps and models described in their article and the same public data sets of EEG Signals. Epilepsy affects more than 65 million people globally, and EEG Signals are critical to analyze and recognize epilepsy. Although the efforts in the last years, it is still challenging to extract useful information from these signals and select useful features in a diagnostic application. We construct a deep convolution network and autoencoders-based model (AE-CDNN) in order to perform unsupervised feature learning. We use the AE-CDNN to extract the features of the available data sets, and then we use some common classifiers to classify the features. The results obtained demonstrate that the proposed AE-CDNN outperforms the traditional feature extraction based classification techniques by achieving better accuracy of classification.

\section{Introduction}

Epilepsy is a chronic neurological disorder, and it is becoming one of the most common neurological diseases in the world \cite{most_commum:2002}. Approximately $1\%$ of the world's population is affected by epilepsy representing more than 65 million people affected \cite{global-epilepsy:2019,Epilepsia:2010}. This disorder is characterized by the occurrence of spontaneous convulsions due to the abnormal synchronous firing of the cortical neurons \cite{stafstrom2015seizures}. This physical reaction can generate many problems for patients, including physical harm caused by the loss of consciousness, shame and discrimination \cite{thomas2011confronting}.

Frequent seizures are dangerous conditions because, at the moment of disruption of the body can occur falls, fractures, burns, car accidents, and other serious physical injuries \cite{mollaouglu2013injuries}. Epilepsy can be defined as a permanent predisposition in the brain to cause epileptic seizures \cite{stafstrom2015seizures}.

To be diagnosed with epilepsy, the patient must have at least two seizures that are caused by comorbidities known in the medical literature, such as: extremely low blood sugar \cite{schauwecker2012effects}. Even when correctly diagnosed and treated, the epileptic patient still suffers side effects and sporadic seizures. The epileptic seizures can cause even irreversible damage to the brain, and then we can visualize the importance of analyzing epilepsy to improve the life quality and the medical treatments for these patients \cite{stafstrom2015seizures}. 

To confirm the diagnostic, epileptologists should generally inspect the long-term electroencephalograms (EEG) of the scalp visually. EEG is a measure of the voltage fluctuation generated by the ion current of neurons in the brain, which reflects the activity of the brain’s bio-electricity and may contain many physiological and disease information \cite{niedermeyer2005electroencephalography}. 

After the discovery that during a patient's seizure the brain activity changes, the EEG has become the most common epilepsy diagnostic tool. Many studies have been made, and the general problem consists in acquiring methods to classify the patients' EEG signals efficiently \cite{puce2017review}. 

However, this costly task still presents several challenges for automatic crisis detection, among them: The scarce number of public data sets; The lack of standardization in seizure classification methodologies; The lack of standardization of data preprocessing; The cost of a specialist to label time intervals; The unbalance of the time series given the rare occurrence of the event; The difficulty of reproducing the works in the literature \cite{craik2019deep}.

With this problem in hand, this paper reproduces the results obtained in \cite{WenZha:2018}, with public data labeled and preprocessed. In addition, we get new results by combining the proposal classifiers into a classifier by set voting, and we add new metrics.

The remainder of this paper is organized as follows: Section \ref{sec:propose} introduces the methodological proposal employed, and their differences with the work of \cite{WenZha:2018}. Section \ref{sec:metho} lists the experimental validation process using epilepsy datasets. Section \ref{sec:resu} presents the corresponding results and analyzes our approach. Finally, conclusions were summarized in Section \ref{sec:concl}.







\section{Methodology Proposal}
\label{sec:propose}

In this section, we describe implementation details, as the core is the reproducible aspect of our reference article. We introduce the idea and implementation of autoencoder/feature learning and our version of the model in \cite{WenZha:2018}, explaining the differences we have made in the original model.

In our study, we keep the autoencoder and feature learning as proposed. However, in the classification, in addition to the individual classifiers, we also employ a large ensemble learning classifier, which decides by majority vote the object class.

\subsection{Implementation Details}

We decided to reproduce the implementation described in the article using Keras \cite{chollet2018keras} and backend in TensorFlow \cite{tensorflow}. Our repository includes a list of all the required libraries employed in acquiring the datasets and running the model (the original and the proposed one). According to the methodology proposed in \cite{Fuente:2019}, we store all the checkpoints for the trained models, for reproduction purposes. Besides that, the training logs can be visualized using TensorBoard tool.

Given the lack of information about implementation in the original paper, some assumptions or cuts are made: 

\begin{itemize}
\item The number of epoch in the AutoEnconder is assumed to be $5000$; 
\item The number of samples per batch size is assumed to be $256$; 
\item A column of the Bonn University EEG dataset was removed since the authors of \cite{WenZha:2018} used $4096$ instances and in this database there is $4097$ instances. The removed attribute is at the endpoint of each object; 
\item In the Children's Hospital of Boston EEG database we use the channel reported by the author to train the AutoEncoder;
\item The loss function presented in equation $12$ in \cite{WenZha:2018} was implemented and we also compared the result obtained with \textit{MAPE}; 
\item The value of the seeds selected in all classifiers, data splitting and elsewhere was $42$; 
\item The train-validation ratio was $80\%-20\%$ to AutoEnconder, in classifiers we use cross-validation with 5 or 10-fold; 
\item Given a sizing problem, we resized the values using the MinMax method, before the classification process.
\item The classifier presented in the final subsection (NN2) was not reproduced for lack of information; 
\end{itemize}

The experiments were performed using a CPU with Intel Core i7-5930K with 3.50 GHz and two GPUs: Nvidia Quadro K5200 and GeForce GTX 970. Some experiments were also run using Nvidia Titan X.

\subsection{AutoEncoders}

The autoencoder implemented is a specific case of neural network structure. It is formed by three layers, an input layer, an output layer and a hidden layer. The training is done to set the weights of the hidden layer to force the input layer and output layer to be as close to each other as possible. Our features are extracted from the hidden layer, which reduces the dimension of data. For more details about the encoding and decoding functions see the Section 2 in the original article \cite{WenZha:2018}.

\subsection{Feature Learning Model}

In this subsection, we will omit equations and minor details (for complete information, see \cite{Shoeb,emami2019autoencoding}. Since we have the dimension reduced by autoencoder we focus on the next challenge: how to obtain effective features from EEG signals. The AE-CDNN implemented follows the steps:

\begin{itemize}
\item Encoder: sample input, convolution layer, down-sampling layer, reshape operation, full connection layer, and the feature coding.
\item Decoder: feature coding as input, full 
connection layer, reshape operation, deconvolution layer, up-sampling layer and the reconstruction samples.
\end{itemize}

Basically, the convolution layer acts as our feature extractor. It performs many successive convolution calculations of the input data and the expectation is to maintain the main components of the input data. The pooling layer is a down-sampling method which reduces data dimension. It uses windows to slide and extract the feature maps. These intervals do not overlap each other, and with then we obtain the pooled feature maps. The feature sizes tested were $m \in \{2, 4, 8, 16, 32, 64, 128, 256\}$\footnote{Size $m = 256$ has not been tested in \cite{WenZha:2018}.}.


The convolution and pooling operations can be iterated multiple times. Reshape operation uses the pooled feature maps to construct an one-dimension vector and a full-connection layer to transform this one-dimension vector. 

Considering $x$ as the input and $y$ as the output, now we need to re-transform the one-dimension vector which will generate the $y$ output, recall we want to minimize the difference between $x$ and $y$ and we have the following equation to calculate loss Mean Absolute Error:

$$\text{Loss MAE}= \frac{1}{N} \sum_{i=1}^N |x^{(i)} - y^{(i)}| .$$


In addition, given the possible interpretations in the original text, we have also used/implemented two loss functions, namely Mean Absolute Percentage Error - MAPE and Mean Absolute Average Error - MAAE \footnote{The formula presented in the original article by \cite{WenZha:2018} differs from the MAPE formula, despite having similar intuitions. Thus, we chose to implement this loss equation, and we have not found its use elsewhere.}, that are contained below:


$$\text{Loss MAPE}= \frac{1}{N} \sum_{i=1}^N \frac{|x^{(i)} - y^{(i)}|}{x^{(i)}} .$$

The difference between the loss functions is only in the fact that one takes in the denominator the value per $x^{(i)}$ and the other takes the average ${\bar x^{(i)}}$.

$$\text{Loss MAAE}= \frac{1}{N} \sum_{i=1}^N \frac{|x^{(i)} - y^{(i)}|}{{\bar x^{(i)}}} .$$

Note that we refer to \cite{WenZha:2018}'s AE-CDNN-L1 as {\it Loss-MAE} and to \cite{WenZha:2018}'s AE-CDNN-L2 as {\it Loss-MAAE}

\subsection{Classification}

Since we have extracted the features with reduced dimension, we use supervised learning models on these features in order to classify the EEG signals. We evaluate each classifier and then we compare the results obtained with each one. The classical classifiers used are: K-Nearest Neighbors (K-NN), Support-Vector Machine - Linear Kernel and Radial Basis Kernel (SVM1, SVM2), Decision Tree (DT), Random Forest (RT), Multilayer Neural Network (MLP), Adaptive Boosting Algorithm (ADB) and Gaussian Naive Bayesian (GNB).
 
The proposed modification combines these classifiers and creates a single classifier that decides by voting. In short, the classifiers were combined by ensemble learning, and the result of the classification became the classification most voted by the classifiers.


















\section{Experimental Methodology}
\label{sec:metho}
In this paper, as in our reference paper \cite{WenZha:2018}, we use unsupervised learning method in EEG signals in order to obtain useful features. This process is needed because the original data is high-dimensional. By using the auto-encoder, we can extract features with reduced dimension. As the original authors we may refer to Bonn University EEG database simply as dataset 1 and to Children's Hospital of Boston EEG database simply as dataset 2.

\subsection{Bonn University EEG database}

We can use different approaches to detect epileptic crisis. Then, to acquire a comparative measure, we verify our outputs using the method described in \ref{sec:propose} and the original one showed in \cite{WenZha:2018}. This database is public and was published by \cite{andrzejak}. The study groups were the control, inter-ictal and ictal distributed into five sets (denotated A-E). Each containing $100$ records of $23.6$ seconds duration and frequency of $173.6$ Hz on a single channel, with $12$-bit resolution. Each data segment has 4097 samples. These recordings underwent a pre-processing in which the signals had a band filter between $0.53$ to $40$ Hz. There was also the removal of artifacts such as muscle movements or flicker movements.

Using labels A, B, C, D and E for the subsets, we have that A and B contain records of 5 healthy volunteers. Set A corresponds to open-eye activity and subset B to closed-eye activity. The subsets C and D have signals during the absence (interictal epileptiform activity) of 5 epileptic patients. And E records the signals during epileptic patients' seizure (ictal intervals). According to \cite{kamath2015analysis}, this dataset is a compilation of recordings under different conditions.

\subsection{Children's Hospital of Boston EEG database}

The second database, also public, contains the EEG signals from a Children\`s Hospital of Boston
\cite{Shoeb}. It was recorded by measuring the brain's electrical activity to obtain EEG signals by connecting multiple electrodes to the patients’ scalp. The data incorporates the EEG signals of 23 children with refractory epilepsy.

This database, built in partnership with the Massachusetts Institute of Technology (MIT), has $5$ men and $18$ women between $3$ and $22$ years. The frequency range was $256$ Hz with $16$ resolution bits. Most patients contain $ 23 $ channels and some with $24$ channels. In contrast to the first set of data, we have multiple channels here, then we need to select channels. The selection followed the methodology used in \cite{shoeb2009application}, which analyzes the variance of each patient, and after that, chooses the channel of greater variance to represent that individual. The channel reported by the authors was $\text{FT9-FT10}$.

In the data of the first ten patients, 200 windows of the same size of the control set were chosen from the epileptic patients we choose $200$, with size of $4096$, in the same way of the control group.


\subsection{Performance Measures}

According to \cite{roy2019deep}, most of the state-of-the-art systems for epilepsy use the metrics defined below. The adaptation of these metrics for evaluating our system contributes to fair comparison with state-of-the-art systems. The definitions of these metrics are given in Table \ref{table:metrics}.

\begin{table}[!ht]
\centering
\begin{tabular}{ccccc}
\hline
 \textbf{Acurracy} & \textbf{Precision} & \textbf{Specificity} & \textbf{Sensitivity} & \textbf{F-Measure} \\ \hline
 $\frac{TP+TN}{TP+TN+FP+FN}$ & $\frac{TP}{TP+FP}$ & $\frac{TN}{TN+FP}$ & $\frac{TP}{FN+TP}$ & $\frac{2\cdot Precision \cdot Sensitivity}{Sensitivity+Precision}$\\ \hline
\end{tabular}
\caption{Use of Metrics and Definition in our paper. Only the Acurracy was considered in \cite{WenZha:2018}.}
\label{table:metrics}
\end{table}

\noindent where False Negatives - FN is the number of epileptic cases, which are predicted as control, True Positives - TP is the number of epileptic cases, which are predicted as epileptic, True Negative - TN is the number of control case that is predicted as control and False Positives - FP is the number of control cases that are identified as epileptic by the system. 

In addition, there was also the AUC-ROC (Area Under The Curve - Receiver Operating Characteristic) defined as the cumulative distribution function of the true positive rate vs the false-negative rate denoted by a threshold.


















\section{Results and Discussion}
\label{sec:resu}

In this section, we analyze three analytical approaches. In the first subsection we analyze the variance present in the channels. The second contains the reproduction of all possible tables and figures, with a discussion of the reasons for the differences. In the third we present an extension of the results, evaluating other classification metrics, proposing a new classifier and varying the classifier parameters.

\subsection{Checking the Variance} 

According to the original authors, the choice of the channel in the Children's Hospital of Boston EEG database observed the variance present in the channels. For that, they followed the methodology: 1) calculate the variance of each channel in each sample, and select the channel with the maximum variance for each sample; 2) count these channels; 3) Select the most often channel considering the first $10$ patients.

Following the methodology presented above we found a different channel from the one found in \cite{WenZha:2018}. So we decided to explore other scenarios. Note that the variance can be calculated based on each sample, per person or the full database. Despite the fact the authors in \cite{WenZha:2018} mentioned explicitly they used the variance of each sample, since we found a different result we explored the other possibilities (per person and the full database).

Thereby, we model the three scenarios. In the first, we analyzed each recording file of the dataset as a sample, having an average length of $921600$ referring to the recorded $3600s$. In these files, we compute and list the electrode with more variance and discard the rest. We accumulate and count for all files. The results obtained can be seen in the Figure below:

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/variance_per_file.pdf}
  \caption{Accumulated variance per sample, considering a sample as each recording file.}
  \label{fig:variance_per_file}
\end{figure}

In the second scenario, we understand that each sample is accumulated per person with all his recordings. So the variance was calculated in parallel in the files and combined for each person. For each person, we count the occurrence of the channel with more variance. As shown in Figure \ref{fig:variance_per_person}.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/variance_per_person.pdf}
  \caption{Accumulated variance per sample, considering a sample as being all the recordings of each person.}
  \label{fig:variance_per_person}
\end{figure}



Finally, as a final scenario, we calculate the cumulative variance across all people and all records, thus, we did not perform a sampling process. In other words, we put all the files together and calculate the variance as if it were a single record. For this, we compute the variance, number of points and average per channel in each file and accumulate through the cumulative variance calculation algorithm. The result of this analysis approach can be seen in Figure \ref{fig:variance_all_files}.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/variance_all.pdf}
  \caption{Total accumulated variance in the first ten people of the dataset, as granular as possible.}
\label{fig:variance_all_files}
\end{figure}


The results obtained in the first, second and third scenarios were not consistent with those reported by the author, in any of them we found the channel FT9-FT10 as the one appearing most for the first $10$ patients. There is also another possible scenario, however, it is not reproducible, where the authors randomly sampled the dataset and evaluated the variance.

Given the associated uncertainty, we decided to repeat the choice of the channel $FT9-FT10$, although this is not the one with the most variance in the modeled scenarios.


\subsection{Reproduction of the values reported by the original author}

The results obtained in our reproduction experiment, for the first dataset, are presented in Accuracy Tables \ref{table:accuracy_boon_mae-reproduction} and \ref{table:accuracy_boon_maae-reproduction} and the differences between results can be seen in Figures \ref{fig:acc-AE-CDNN-MAE-d1} and \ref{fig:acc-AE-CDNN-MAAE-d1}. We employed the same methodology as the one used in the original paper, performing a $5$-fold cross-validation for each classifier, and we show the mean values. For each table reproduced, we also present the original result and the difference between them.


\input{table/accuracy_boon_mae-reproduction.tex}

\input{table/accuracy_boon_maae-reproduction.tex}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/table_2.pdf}
  \caption{Classification Accuracy Results of AE-CDNN-MAE for Dataset 1 \cite{WenZha:2018}, Reproduced Original and Difference.}
\label{fig:acc-AE-CDNN-MAE-d1}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/table_3.pdf}
  \caption{Classification Accuracy Results of AE-CDNN-MAAE for Dataset 1 \cite{WenZha:2018}, Reproduced Original and Difference.}
\label{fig:acc-AE-CDNN-MAAE-d1}
\end{figure}

We can perceive some differences when compared to the original results. In Table \ref{table:accuracy_boon_mae-reproduction}, we acquired the best average with a dimension equal to 64, while the original document acquired the best average when the dimension is equal to 128. The original document acquired higher accuracy values in most cases, even though when the dimension is equal to 2 or 4, our accuracy values are higher. The best precision in our article and in the original article was obtained by the random forest algorithm.


\input{table/accuracy_chbmit_mae-reproduction.tex}

\input{table/accuracy_chbmit_maae-reproduction.tex}



\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/table_4.pdf}
  \caption{Classification Accuracy Results of AE-CDNN-MAE for Dataset 2 \cite{WenZha:2018}, Reproduced Original and Difference.}
\label{fig:acc-AE-CDNN-MAE-d2}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/table_5.pdf}
  \caption{Classification Accuracy Results of AE-CDNN-MAAE for Dataset 2 \cite{WenZha:2018}, Reproduced Original and Difference.}
\label{fig:acc-AE-CDNN-MAAE-d2}
\end{figure}


\newpage

Considering Dataset 2 and Tables \ref{table:accuracy_chbmit_mae-reproduction}, \ref{table:accuracy_chbmit_maae-reproduction} we acquired similar results when compared with the results obtained by the original authors \cite{WenZha:2018}. In general, the original paper acquired a maximum accuracy greater than those obtained by our reproduction implementation, but the average and unique values per dimension are close in most cases considering both functions AE-CDNN-MAE and AE-CDNN-MAAE. However, for Dataset 1 the accuracy values obtained in this paper are significantly lower than the ones obtained by the original paper considering both AE-CDNN-MAE and AE-CDNN-MAAE, as shown in Figure \ref{fig:average}. 

\newpage


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/average-MAE-MAAE.pdf}
  \caption{Average Accuracy Results of AE-CDNN-MAE and AE-CDNN-MAAE, with different dimension values in the two dataset.}
\label{fig:average}
\end{figure}

%As already shown in the Tables, the results obtained in the reproduction were similar. 
When analyzing the behavior of the different loss functions, we see that the MAAE function does not always obtain superior results than the MAE function. The same is observed in the original article, as well as a similar behavior, but the average accuracy obtained are significantly higher than those obtained by our reproduction. Similarly, when we analyze the loss function MAAE and MAPE, in Figure \ref{fig:average-maae-mape}, we have that the behavior of both is not very divergent, being MAAE generating a higher accuracy in the first dataset. In the second dataset, MAPE has a more stable behavior and generates greater accuracy. 


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/average-MAAE-MAPE.pdf}
  \caption{Average Accuracy Results of AE-CDNN-MAAE and AE-CDNN-MAPE in the two dataset.}
\label{fig:average-maae-mape}
\end{figure}

When observing the values obtained in the classification, in the k-fold, we have that the accuracy values follow the proportion of the data, indicating the non-learning of the classification methods. We observed below the result for the accuracy inspection, for the cross validation, for $m = 2$. Analyzing the accuracy obtained by classifiers in Tables \ref{table:acc-mae-boon-fold-2} and \ref{table:acc-maae-boon-fold-2} we observe the values obtained by AE-CDNN-MAAE and AE-CDNN-MAE are close, however the function AE-CDNN-MAAE acquired smoothly better results and with less variation, in general. 

\input{table/acc-mae-boon-fold-2.tex}

\input{table/acc-maae-boon-fold-2.tex}

In the original paper we observe similar differences between the two functions, the results for AE-CDNN-MAAE are smoothly better for most classifiers, but considering \textbf{gaussian\_nb}, for example, the function AE-CDNN-MAAE acquired much better results comparing with AE-CDNN-MAE. Although the results in original paper also have few variations for the classifiers \textbf{svm\_linear}, \textbf{svm\_radial} and \textbf{multi\_layer} we had no variance in these classifers for function AE-CDNN-MAAE.

In the second dataset, in Tables below, when analyzing by fold we have that results are worse than those reported by the authors. However, the results is consistent with the hypothesis during the process, there was no feature learning. Also given the balance of this second dataset, we have that all methods do not present a better result than the random chance. 

\input{table/acc-mae-chbmit-fold-2.tex}

\input{table/acc-maae-chbmit-fold-2.tex}

When analyzing the reduced values by class, specifically with $m = 4$ we have $3$ of the $4$ attributes are $0$, in the best scenario, indicating that there was no learning in Auto Encoder to distinguish the behavior by class. This bad representation of latent space occurs regardless of the loss function.


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/feature_distribution_4.pdf}
  \caption{Feature Distribution of AE-CDNN-MAE and AE-CDNN-MAAE, with $m=4$, in the first dataset. }
\label{fig:feature_distribution_4}
\end{figure}

\newpage

When we analyze the behavior of the loss functions at the epoch, in Figure \ref{fig:change_loss_mae_maae}, in the first dataset, we have that these do not have a parallel with those reported by the original author. In addition, numerically in the second function MAAE the values also do not present an adequate dimension with that originally reported. Consequently, we also have an indication that in the MAAE function there was not an adequate generalization in the validation.


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/change_loss_mae_maae.pdf}
  \caption{Change of loss function of AE-CDNN-MAE and AE-CDNN-MAAE, in the first dataset, with $m=4$.}
\label{fig:change_loss_mae_maae}
\end{figure}


Even assuming that the author used the MAPE loss function, we still do not obtain an adequate result in loss at the epoch, as show the Figure \ref{fig:change_loss_mae_mape}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figure/change_loss_mae_mape.pdf}
  \caption{Change of loss function of AE-CDNN-MAE and AE-CDNN-MAPE, in the first dataset, with $m=4$.}
\label{fig:change_loss_mae_mape}
\end{figure}


These differences also occur in the establishment in the baseline methods, indicating that there is some cut in the training set that was not included in this modeling, given the lack of information in the article. In Figure \ref{fig:baseline_methods} we observe similar average accuracy between AE-CDNN-MAE, AE-CDNN-MAAE, PCA and SRP for both datasets. 


\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figure/baseline_methods.pdf}
  \caption{Comparison of accuracy for different loss functions (AE-CDNN-MAE, AE-CDNN-MAAE), and also with baseline (PCA, SRP).}
\label{fig:baseline_methods}
\end{figure}

The same is observed in the original article, as well as a similar behavior, but the average accuracy obtained for Dataset 1 are significantly higher than those obtained by our reproduction, as shown in the Table \ref{table:metrics_boon_cv_10}:

\input{table/metrics_boon_cv_10.tex}

When we analyze the result assuming a $10$-fold, we have an increase in the accuracy values for the first dataset, however, still below that reported by the author.

\subsection{Extension of the values reported by the original author}
 
In Precision Tables \ref{table:precision_boon_maae-reproduction}, \ref{table:precision_boon_maae-reproduction}, \ref{table:precision_chbmit_mae-reproduction} and \ref{table:precision_chbmit_maae-reproduction}, we realize that one of the most precise and specific method was the Gaussian Naive Bayesian; however, when analyzing the behavior in the Sensitivity metric, we do not have a satisfactory result. This indicates that the method pinpoints true negatives rather than true positives. If treated from a medical field, this result is worrying. The cases that the method indicates are true positives; however, this method misses many cases.

We also analyze that we cannot consider Support Vector Machine (Linear and Radial) or Multi-Layer results with the lowest $m$. The result in specificity indicates that the method behaves unwanted, possibly indicating all values as true positives. This rule burdens the medical system because further detection of the seizure requires further investigation for a complete diagnosis of the disease.

By our method, we note that the three metrics indicate that a progression in the number of features generates an improvement in seizure detection. Similar behavior is observed in Multi-Layer, only for Accuracy and Sensitivity, in this case, a beneficial behavior for the application. No trend was observed in the other methods. The average of the methods does not exceed our Ensemble method in almost any scenario. 


\input{table/precision_boon_mae-reproduction.tex}

\input{table/precision_boon_maae-reproduction.tex}


In the first dataset, when we analyze the accuracy we have that the naive bayes Gaussian classifier presents a drop of ($ 40 \%, 23 \% $, for first and second loss respectively) if compared to the accuracy. The average difference, in precision minus accuracy, is $6\%$, indicating that the precision metric achieves slightly higher results on average in the samples. The k\_neighbors classifier is the classifier, in the first loss function, that has the least average difference in results, while the svm\_linear method shows the same result for the second loss set. At the other end, we have the largest variation in both gaussian\_nb data sets.


\input{table/precision_chbmit_mae-reproduction.tex}

\input{table/precision_chbmit_maae-reproduction.tex}

Meanwhile, in the second set of data, generated by the two loss functions, the difference between precision and accuracy is greater in the smallest dimensions, while the values are more stable, and close to the accuracy values in the largest dimensions. Such stability behavior is also observed in the absolute values.


\subsubsection{Specificity and Sensitivity}

When we analyze the specificity, we have that the SVM method, with different kernels, cannot obtain a separation of the hyperspaces of the attributes to distinguish the non-schizoid events. In this way, we have that the classifier cannot distinguish when the person is without epileptic attack. From a medical point of view, there are not so many implications for this, since the weighting of importance is inclined to detect true positives. The SVM sensitivity for these cases, in high dimensions (above $ 32 $) presents reasonable values, approximately $70\%$ in the worst scenarios. In general, the panorama of the accumulated sensitivity indicates that the worst classifiers, regardless of the number of dimensions, are the Gaussian naive bayes, and the \textbf{K}-neighbors for high dimensions. The ensemble classifier has average cumulative sensitivity ($ 71 \% $ in the worst case scenario), with the exception of lower case scenarios $ 2 $.

The performance of the Gaussian classifier may be related to the fact that the inputs are highly dependent on each other, thus violating the method's premise of independence. In the case of the k-neighbors classifier, given the presence of the values $ 0 $ in various dimensions, as shown previously, which can affect the distance assumptions necessary for the method.



\input{table/specificity_boon_mae-reproduction.tex}

\input{table/specificity_boon_maae-reproduction.tex}


\input{table/specificity_chbmit_mae-reproduction.tex}

\input{table/specificity_chbmit_maae-reproduction.tex}



\input{table/sensitivity_boon_mae-reproduction.tex}

\input{table/sensitivity_boon_maae-reproduction.tex}

\input{table/sensitivity_chbmit_mae-reproduction.tex}

\input{table/sensitivity_chbmit_maae-reproduction.tex}



\newpage

\subsubsection{F-measure and ROC-AUC}

When analyzing the behavior of the methods for the reason of the metrics of $ F-measure $ and $ ROC-AUC $ we have that in the first data set, the SVM and multi\_layer methods present the best results. At the other end, we have the appearance with the naive bayes and $k$ neighbor methods.

We analyze the behavior of the F-measure. The relationship between sensitivity and precision is captured by this measure, as the gaussian\_nb, svm\_linear, k\_neighbors methods did not obtain good results, which is coherent with the accuracy result. The measures generally show close results with each other. 



\input{table/f-measure_boon_mae-reproduction.tex}

\input{table/f-measure_boon_maae-reproduction.tex}

\input{table/f-measure_chbmit_mae-reproduction.tex}

\input{table/f-measure_chbmit_maae-reproduction.tex}


\input{table/roc-auc_boon_mae-reproduction.tex}

\input{table/roc-auc_boon_maae-reproduction.tex}

\input{table/roc-auc_chbmit_mae-reproduction.tex}

\input{table/roc-auc_chbmit_maae-reproduction.tex}





\newpage

\section{Conclusion}\label{sec:concl}
In this article, we re-implemented the approach proposed in \cite{WenZha:2018} and propose the use of a different classifier. This classification approach, based on deep learning for detecting epileptic seizures using EGG had not been explored previously. We adopted a Auto-Encoder that allowed us to construct a smaller representation space. Among the variety of metrics, using the ensemble method results in better ROC-AUC results.

The original authors left some gaps that made it impossible to fully reproduce their obtained results. For example, the lack of information about the neural classifier used in the last sub-section, about the sampling process of the first and second data sets, and the number of times or batch size. Consequently, the results obtained can be considered at most a replication.

As a second contribution, the developed codes can be easily ported to other tasks. Moreover, it could be used to evaluate other variants of the neural network architecture, techniques for classifying the signals, data augmentation, among other possibilities.  



